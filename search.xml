<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[星号运算符在python中的用法]]></title>
    <url>%2F2019%2F01%2F14%2F*%E8%BF%90%E7%AE%97%E7%AC%A6%E5%9C%A8python%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%8E%8B%2C%E4%BC%A0%E5%8F%82%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[*运算符在python中的解压,传参用法 注:本文首发于我的知乎文章 这两天得了一本《Python Cookbook》，甚是喜爱，希望能记录一点有意思的Python 用法，既是作为自己的读书笔记，让自己能够不断精进技术，也是为了能够和大家一起讨论Python。 从zip说开去*操作符在Python中除了乘法和复制(如[1] * 5)的用法之外，还有解压(unpack)的功能，这应该是所有用过zip()的人都知道的事情。zip函数的基本用法如下: 12345stuff = ['apple','banana','peach']money = [10, 5, 7]pair = list(zip(stuff,money))# pair = [('apple',10),('banana',5),('peach',7)] 但是如果我们现在已经有 pair 这个 list 了，希望能够还原成stuff 和 money 两个 list,我们就需要用到*符号:1stuff,money = zip(*pair) 在这里*就起到了解压的效果 *对于迭代对象的作用当然了，*并不是说只能与 zip() 联合使用，事实上，任何一个函数都可以用到*符号，只要它操作的对象是可迭代的:12345678910111213141516def do_something(x,y,z): print('x is ', x) print('y is ', y) print('z is ', z) list1 = ['a','b','c']&gt;&gt;&gt; do_something(list1)Trackback (most recent call last): File "&lt;stdin&gt;", line1, in &lt;module&gt;TypeError: do() missing 2 required positional arguments: 'y' and 'z'&gt;&gt;&gt; do_something(*list1)x is ay is bz is c 可知，*操作符自动将 list1 中的元素赋给了三个形参，这就是*很方便的用法 **的用法当然，对于字典(dict)来说，我们也可以使用*运算符:123456dict1 = &#123;'x':1, 'y':2, 'z':3&#125;&gt;&gt;&gt; do_something(*dict1)x is xy is yz is z 但是这样子得到的是 key 值，如果想要得到 value 值，我们需要使用**运算符:1234&gt;&gt;&gt; do_somthing(**dict1)x is 1y is 2z is 3 一定要注意的是，此处的用法必须要求函数形参与字典 key 值一一对应，请看下面的用法:123456789dict2 = &#123;'z':1, 'x':2, 'y':3&#125;&gt;&gt;&gt; do_something(**dict2)x is 2y is 3z is 1dict3 = &#123;'a':1, 'b':2, 'c':3&#125;&gt;&gt;&gt; do_something(**dict3)# TypeError: do_something() missing 1 required positional argument: 'x' 实际上，**的用法应该这么理解: **dict2 等效于 z=1, x=2, y=3， 因此 do_something(**dict2) 等效于 do_something(z=1, x=2, y=3), 这样看来，dict3的错误就是必然的了，因为函数没有 a 这个参数 为了加深理解，请看下面这条例子(来自Reference 2):1234&gt;&gt;&gt; date_info = &#123;'year': "2020", 'month': "01", 'day': "01"&#125; &gt;&gt;&gt; filename = "&#123;year&#125;-&#123;month&#125;-&#123;day&#125;.txt".format(**date_info) &gt;&gt;&gt; filename'2020-01-01.txt' 理解 *args 和**kwargs*args和**kwargs是我们经常见到的两个参数，但这两个参数究竟是干什么的可能有些人仍有疑惑。其实它们的用法也很简单，对于一个函数而言，如果你不确定它还有什么待传入的参数，就不妨用一个*args(当然你不一定非得叫args，但一般都喜欢起这个名字)，它将输入的多余形参以元组形式保存在args中:12345678910111213141516171819202122232425#两个数的加法def adder_2(x,y): return x+y#三个数的加法def adder_3(x,y,z): return x+y+z# 无限个数的加法def adder_unlimited(*args): result = 0 for num in args: result += num return result&gt;&gt;&gt; adder_unlimited(1)1&gt;&gt;&gt; adder_unlimited(1,2)3&gt;&gt;&gt; adder_unlimited(1,2,3,4)10&gt;&gt;&gt; list_num = [1,2,3,4]&gt;&gt;&gt; adder_unlimited(*list_num) #活学活用10 **kwargs效果则是将输入的未定义的形参及其对应的值存在kwargs字典里(例子来源Reference 4):12345678910111213141516171819202122def intro(**data): print("\nData type of argument:",type(data)) for key, value in data.items(): print("&#123;&#125; is &#123;&#125;".format(key,value))&gt;&gt;&gt; intro(Firstname="Sita", Lastname="Sharma", Age=22, Phone=1234567890)Data type of argument: &lt;class 'dict'&gt;Firstname is SitaLastname is SharmaAge is 22Phone is 1234567890&gt;&gt;&gt; intro(Firstname="John", Lastname="Wood", Email="johnwood@nomail.com", Country="Wakanda", Age=25, Phone=9876543210)Data type of argument: &lt;class 'dict'&gt;Firstname is JohnLastname is WoodEmail is johnwood@nomail.comCountry is WakandaAge is 25Phone is 9876543210 因此，对于一个好的 API 来说，应该尽量使用*args和**kwargs以提高程序稳定性 利用*从可迭代对象中分解元素说了这么多终于说到了《Python Cookbook》中的内容了(内心OS:累死我了)。 假设我们现在要对一个歌手的表现打分，去掉一个最高分，去掉一个最低分，对其他所有分数（未知个数）求平均分，这时*方法就派上用场了:123def drop_first_last(grades): first, *middle, last = grades return avg(middle) 也许你会说，用切片也可以达到同样效果，如middle = grades[1:-1]，但如果我们需要用到first和last呢？你是不是还要first = grades[0]和last = grades[-1]呢？ 太不优雅了！ 事实上，下面这个例子如果用切片来写就十分麻烦(来源 Reference 3): 12345678&gt;&gt;&gt;line = 'nobody:*:-2:-2:Unprevileged User:/var/empty:/usr/bin/false'&gt;&gt;&gt; uname,*_, homedir, sh = line.split(':')&gt;&gt;&gt; uname'nobody'&gt;&gt;&gt; homedir'/var/empty'&gt;&gt;&gt; sh'/usr/bin/false' 其中*_将我们 split 之后不想要的部分直接丢弃了(一般用下划线来为我们不需要的变量取名) 实际上，*操作和函数式语言中的列表处理功能相似，相信学过Coursera 华盛顿大学神课Programming Language的同学对这一形式一定不会陌生(来源 Reference3): 123def sum(items): head, *tail = items return head + sum(tail) if tail else head 当然，限于Python自身内部对递归的次数限制，这个例子在实践中意义不大，但是它确实是十分精妙，令人不禁击节赞叹！ 其他奇技淫巧还有一些我不知道怎么分类的小技术，在这里也一并献给大家(例子来自 Reference 2): 列表元素快速换位置1sequence = [*sequence[1:], sequence[0]] #将首个元素换到最后一个 将多个迭代对象转化为 list很多时候，我们在使用完一个函数时，他返回的值都是generator 等迭代对象，必须套上一个list()才可以变为列表，如zip()，reversed()等，此时如果每个函数都套上list()则显得不够优雅，更好的方式如下:12345def palindromify(sequence): return list(sequence) + list(reversed(sequence)) #不够优雅def palindromify(sequence): return [*sequence, *reversed(sequence)] # better 同理，多个 list 的拼接也可以如法炮制:1list_all = [*list1,*list2,*list3,*list4] 字典的合并同理，字典的合并用**也是十分方便:1234&gt;&gt;&gt; date_info = &#123;'year': "2020", 'month': "01", 'day': "01"&#125; &gt;&gt;&gt; track_info = &#123;'artist': "Beethoven", 'title': 'Symphony No 5'&#125; &gt;&gt;&gt; all_info = &#123;**date_info, **track_info&#125; &gt;&gt;&gt; all_info &#123;'year': '2020', 'month': '01', 'day': '01', 'artist': 'Beethoven', 'title': 'Symphony No 5'&#125; 最后的闲言碎语第一次写技术博客，竟然写了三个小时，大概算是涵盖了大部分*的用法，希望这是一个好的开始，也希望所有读者能多提些宝贵意见，谢谢！ 之后我还想补充一下Python中*是如何实现的，不过这个可能得去翻源码才能有所斩获吧！ Reference Python Zip() Asterisks in Python: what they are and how to use them Python Cookbook Python *args and **kwargs]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[GSOC Final Submission]]></title>
    <url>%2F2018%2F08%2F14%2F%2Farchivers%2FGSOC-Final-Submission%2F</url>
    <content type="text"><![CDATA[I have finished the GSOC journey during the last 3 months. Thanks for the help from Prof. Francis Steen and Prof. Mark Turner and all the people from Red Hen Lab who helped me overcome so many problems and difficulties. By the end of the program, I build up a Chinese Pipeline which could extract the text from the videos(offered by Red Hen Lab, mostly Chinese news) automatically so that we could do natural language processing tasks afterwards. Technically, I learned HPC, singularity and all the container concepts, how to set up a singularity environment via recipe, how to write shell scripts, how to organize an open source project and some more details about the implementation of DeepSpeech2 from Baidu. Some Links about my work Github: https://github.com/RedHenLab/ASR-for-Chinese-Pipeline Red Hen Lab page: http://www.redhenlab.org/home/the-cognitive-core-research-topics-in-red-hen/chinese-pipeline My personal website: xuzhaoqing.github.io Introduction about Chinese PipelineMy project is to implement a pipeline to extract the text of the voices in the videos(mostly Chinese TV news) from Red Hen Lab. The task is most related to Autimatic Speech Recognition. What I have done I read all of the paper and codes in Baidu Deepspeech2 in detail and use part of their tools and pretrained models to fit our needs. You could go here to see their incredible work: Deepspeech2 I got used to using singularity and upload the image that we use to singularity hub: image so that everyone, no matter if they were in Red Hen Lab or not, could use this image to do their ASR tasks following our instructions. To know the differences between singularity,docker and shifter, here is a great article: http://geekyap.blogspot.com/2016/11/docker-vs-singularity-vs-shifter-in-hpc.html I did the preprocessing of our data, including convert videos to audios, spliting them, resampling and so on. You could check Prepare the data I and Prepare the data II to see the details. After that, I first experimented the project on a video so that I could write the python code, shell scripts and parameters tuning. It cost me a lot of time and you could see the result in the example/output/1.txt After that, I put it into production with all of our data available, a little of change on code could satisfy your requirement for your task. You could see the main implementation in code folder. Finally, I write all the docs, wikis and comments of the code. To be continued Currently the code in real production is not efficient. I have to implement monitor-new-data function for it, and mutli-thread techniques will help me save a lot of time. Use the data to do some easy nlp tasks like word segmentation, word counts and so on. It would be easily done in 1 or 2 days. aligments would be a nice task to try. Reproduce my workGet into the serveronly for people in Red Hen Lab But if you have a similiar HPC environment, it’s still useful for you to know.123456module load singularity/2.5.1module load cuda/7.5export SINGULARITY_BINDPATH=&quot;/mnt&quot;srun -p gpu -C gpup100 --mem=100gb --gres=gpu:2 --pty bashcd /mnt/rds/redhen/gallina/Singularity/Chinese_Pipelinesingularity shell -e --nv Chinese_Pipeline.simg or if you don’t want to know the details and just want to check our work, just check /mnt/rds/redhen/gallina/Singularity/Chinese_Pipeline/srun.out The ExampleBefore we really put it into production, let’s look at an example to see how it works clearly.Firstly, let’s get into the example folder:1cd example here we can find a mini structure of our program(or actually the core of our program):12345[zxx302@hpc3 example]$ lscompute_mean.sh EX.wav manifest.py params.tar.gzcompute_mean_std.py infer.py mean_std.npz splitdata_utils infer.sh model_utils utilsdecoders manifest.ex output vocab.txt You could find a detailed explanation of these files in: Here let’s just see the result:12345678910111213[zxx302@hpc3 example]$ vim output/1.txt一二一运动与人文文名人名言对公众网上和上行今年十一月二十号五十六有小雨今天是二十世纪中的行为受美国的一年节目的主要有中共中央召开党外人士座谈会征求对中共中央关于修改宪法部分内容的一天习近平主持并发表重要讲话张德江俞正声李战书汪洋蒙古民主席写报道在一新时代通过合作社会主义思想的影响今天来看贫困县最多的云南省是有中国百姓新的透明质感空间感确保给群众出摆脱贫困实现共同和分别是创新方法防治大气污染南京市吸纳顶尖人才抢占战科技制高点系列报道也时代新气象新作为影片提供机构的炮击是美国支持在叙利亚会的无声人民保护部队叙利亚政府警告方抚养条件美国国会参议院未能通过联保证处理事故美国联保... We can see that even there are several typos here, it’s still above the average. The resultsYou could find the results of ASR in the text folder under the Chinese_Pipeline. It’s the output by executing code/infer.sh. AcknowledgementHere I would like to thank all of the support and help from my mentors in Red Hen Lab, especially Prof.Steen and Prof.Turner. They give me consistent support along the summer. And I would like to thank Baidu for their kindness to open source such a great tool for us to use as well as give a clear documentation.]]></content>
      <categories>
        <category>GSOC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prepare the Data 2]]></title>
    <url>%2F2018%2F07%2F07%2F%2Farchivers%2FPrepare-the-Data-2%2F</url>
    <content type="text"><![CDATA[Here I will write some shell scripts to automatically find all the Chinese videos in the datasets and transform them into the form that could be input in our DeepSpeech2 model. Find the Chinese dataThe data is all stored in the /mnt/rds/redhen/gallina/tv/ directory, including English,French,Chinese and so on. To automatically find all the Chinese data from 2018-01 to 2018-06, I write a shell script called extract.sh:12345678910111213#!/bin/shcd /mnt/rds/redhen/gallina/tv/2018for date in 2018-0&#123;1,2,3,4,5,6&#125;do cd $date for file in `ls` do find ./$file -name '*_CN_*.mp4' -exec cp &#123;&#125; /mnt/rds/redhen/gallina/Singularity/data/$date \; echo $file done cd ..doneexit Change from mp4 to wavTo get the audio information from the videos, I create a shell script called transform.sh:123456789101112131415161718192021222324252627282930#!/bin/shcd /mnt/rds/redhen/gallina/Singularity/datafor file in `ls`do if [ "$file" != "wav" ] then for mp4 in `ls $file` do ffmpeg -i ./$file/$mp4 wav/$file/$&#123;mp4%%.*&#125;.wav done fidone``` ## Split the dataWe have to split them into 10 seconds each to fit the input form. To automatically split them, we could use my shell script `split.sh`:```bash#!/bin/bashcd /mnt/rds/redhen/gallina/Singularity/data/wavecho 'Split Begins'for file in `ls`do for wav in `ls $file` do mkdir -p ../split/$file/$&#123;wav%%.*&#125; sox $file/$wav ../split/$file/$&#123;wav%%.*&#125;/$wav trim 0 10 : newfile : restart \; echo $file/$wav' split done' donedoneexit After that, we could find all the data in /mnt/rds/redhen/gallina/Singularity/data/split. all the wav files splitted from one video are placed into one folder with the same name of their video. Take one for example:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[zxx302@hpc3 split]$ cd ../data/split[zxx302@hpc3 split]$ ls2018-01 2018-02 2018-03 2018-04 2018-05 2018-06[zxx302@hpc3 split]$ ls 2018-012018-01-08_0410_CN_HNTV1_午间新闻2018-01-09_0410_CN_HNTV1_午间新闻2018-01-09_1100_CN_CCTV1_新闻联播2018-01-09_1359_CN_CCTV1_晚间新闻2018-01-09_1500_CN_CCTV1_24小时2018-01-09_2033_CN_CCTV13_新闻1+12018-01-09_2300_CN_HNCSTV_Changsha_News2018-01-10_0410_CN_HNTV1_午间新闻2018-01-10_1100_CN_CCTV1_新闻联播2018-01-10_1359_CN_CCTV1_晚间新闻2018-01-10_1500_CN_CCTV1_24小时2018-01-10_2033_CN_CCTV13_新闻1+12018-01-10_2300_CN_HNCSTV_Changsha_News2018-01-11_0410_CN_HNTV1_午间新闻...[zxx302@hpc3 split]$ ls 2018-01/2018-01-26_2033_CN_CCTV13_新闻1+1/2018-01-26_2033_CN_CCTV13_新闻1+1001.wav 2018-01-26_2033_CN_CCTV13_新闻1+1055.wav 2018-01-26_2033_CN_CCTV13_新闻1+1109.wav2018-01-26_2033_CN_CCTV13_新闻1+1002.wav 2018-01-26_2033_CN_CCTV13_新闻1+1056.wav 2018-01-26_2033_CN_CCTV13_新闻1+1110.wav2018-01-26_2033_CN_CCTV13_新闻1+1003.wav 2018-01-26_2033_CN_CCTV13_新闻1+1057.wav 2018-01-26_2033_CN_CCTV13_新闻1+1111.wav2018-01-26_2033_CN_CCTV13_新闻1+1004.wav 2018-01-26_2033_CN_CCTV13_新闻1+1058.wav 2018-01-26_2033_CN_CCTV13_新闻1+1112.wav2018-01-26_2033_CN_CCTV13_新闻1+1005.wav 2018-01-26_2033_CN_CCTV13_新闻1+1059.wav 2018-01-26_2033_CN_CCTV13_新闻1+1113.wav2018-01-26_2033_CN_CCTV13_新闻1+1006.wav 2018-01-26_2033_CN_CCTV13_新闻1+1060.wav 2018-01-26_2033_CN_CCTV13_新闻1+1114.wav2018-01-26_2033_CN_CCTV13_新闻1+1007.wav 2018-01-26_2033_CN_CCTV13_新闻1+1061.wav 2018-01-26_2033_CN_CCTV13_新闻1+1115.wav2018-01-26_2033_CN_CCTV13_新闻1+1008.wav 2018-01-26_2033_CN_CCTV13_新闻1+1062.wav 2018-01-26_2033_CN_CCTV13_新闻1+1116.wav2018-01-26_2033_CN_CCTV13_新闻1+1009.wav 2018-01-26_2033_CN_CCTV13_新闻1+1063.wav 2018-01-26_2033_CN_CCTV13_新闻1+1117.wav2018-01-26_2033_CN_CCTV13_新闻1+1010.wav 2018-01-26_2033_CN_CCTV13_新闻1+1064.wav 2018-01-26_2033_CN_CCTV13_新闻1+1118.wav2018-01-26_2033_CN_CCTV13_新闻1+1011.wav 2018-01-26_2033_CN_CCTV13_新闻1+1065.wav 2018-01-26_2033_CN_CCTV13_新闻1+1119.wav2018-01-26_2033_CN_CCTV13_新闻1+1012.wav 2018-01-26_2033_CN_CCTV13_新闻1+1066.wav 2018-01-26_2033_CN_CCTV13_新闻1+1120.wav2018-01-26_2033_CN_CCTV13_新闻1+1013.wav 2018-01-26_2033_CN_CCTV13_新闻1+1067.wav 2018-01-26_2033_CN_CCTV13_新闻1+1121.wav2018-01-26_2033_CN_CCTV13_新闻1+1014.wav 2018-01-26_2033_CN_CCTV13_新闻1+1068.wav 2018-01-26_2033_CN_CCTV13_新闻1+1122.wav2018-01-26_2033_CN_CCTV13_新闻1+1015.wav 2018-01-26_2033_CN_CCTV13_新闻1+1069.wav 2018-01-26_2033_CN_CCTV13_新闻1+1123.wav2018-01-26_2033_CN_CCTV13_新闻1+1016.wav 2018-01-26_2033_CN_CCTV13_新闻1+1070.wav 2018-01-26_2033_CN_CCTV13_新闻1+1124.wav2018-01-26_2033_CN_CCTV13_新闻1+1017.wav 2018-01-26_2033_CN_CCTV13_新闻1+1071.wav 2018-01-26_2033_CN_CCTV13_新闻1+1125.wav2018-01-26_2033_CN_CCTV13_新闻1+1018.wav 2018-01-26_2033_CN_CCTV13_新闻1+1072.wav 2018-01-26_2033_CN_CCTV13_新闻1+1126.wav2018-01-26_2033_CN_CCTV13_新闻1+1019.wav 2018-01-26_2033_CN_CCTV13_新闻1+1073.wav 2018-01-26_2033_CN_CCTV13_新闻1+1127.wav2018-01-26_2033_CN_CCTV13_新闻1+1020.wav 2018-01-26_2033_CN_CCTV13_新闻1+1074.wav 2018-01-26_2033_CN_CCTV13_新闻1+1128.wav2018-01-26_2033_CN_CCTV13_新闻1+1021.wav 2018-01-26_2033_CN_CCTV13_新闻1+1075.wav 2018-01-26_2033_CN_CCTV13_新闻1+1129.wav2018-01-26_2033_CN_CCTV13_新闻1+1022.wav 2018-01-26_2033_CN_CCTV13_新闻1+1076.wav 2018-01-26_2033_CN_CCTV13_新闻1+1130.wav2018-01-26_2033_CN_CCTV13_新闻1+1023.wav 2018-01-26_2033_CN_CCTV13_新闻1+1077.wav 2018-01-26_2033_CN_CCTV13_新闻1+1131.wav2018-01-26_2033_CN_CCTV13_新闻1+1024.wav 2018-01-26_2033_CN_CCTV13_新闻1+1078.wav 2018-01-26_2033_CN_CCTV13_新闻1+1132.wav2018-01-26_2033_CN_CCTV13_新闻1+1025.wav 2018-01-26_2033_CN_CCTV13_新闻1+1079.wav 2018-01-26_2033_CN_CCTV13_新闻1+1133.wav2018-01-26_2033_CN_CCTV13_新闻1+1026.wav 2018-01-26_2033_CN_CCTV13_新闻1+1080.wav 2018-01-26_2033_CN_CCTV13_新闻1+1134.wav2018-01-26_2033_CN_CCTV13_新闻1+1027.wav 2018-01-26_2033_CN_CCTV13_新闻1+1081.wav 2018-01-26_2033_CN_CCTV13_新闻1+1135.wav2018-01-26_2033_CN_CCTV13_新闻1+1028.wav 2018-01-26_2033_CN_CCTV13_新闻1+1082.wav 2018-01-26_2033_CN_CCTV13_新闻1+1136.wav2018-01-26_2033_CN_CCTV13_新闻1+1029.wav 2018-01-26_2033_CN_CCTV13_新闻1+1083.wav 2018-01-26_2033_CN_CCTV13_新闻1+1137.wav2018-01-26_2033_CN_CCTV13_新闻1+1030.wav 2018-01-26_2033_CN_CCTV13_新闻1+1084.wav 2018-01-26_2033_CN_CCTV13_新闻1+1138.wav2018-01-26_2033_CN_CCTV13_新闻1+1031.wav 2018-01-26_2033_CN_CCTV13_新闻1+1085.wav 2018-01-26_2033_CN_CCTV13_新闻1+1139.wav2018-01-26_2033_CN_CCTV13_新闻1+1032.wav 2018-01-26_2033_CN_CCTV13_新闻1+1086.wav 2018-01-26_2033_CN_CCTV13_新闻1+1140.wav2018-01-26_2033_CN_CCTV13_新闻1+1033.wav 2018-01-26_2033_CN_CCTV13_新闻1+1087.wav 2018-01-26_2033_CN_CCTV13_新闻1+1141.wav2018-01-26_2033_CN_CCTV13_新闻1+1034.wav 2018-01-26_2033_CN_CCTV13_新闻1+1088.wav 2018-01-26_2033_CN_CCTV13_新闻1+1142.wav2018-01-26_2033_CN_CCTV13_新闻1+1035.wav 2018-01-26_2033_CN_CCTV13_新闻1+1089.wav 2018-01-26_2033_CN_CCTV13_新闻1+1143.wav2018-01-26_2033_CN_CCTV13_新闻1+1036.wav 2018-01-26_2033_CN_CCTV13_新闻1+1090.wav 2018-01-26_2033_CN_CCTV13_新闻1+1144.wav2018-01-26_2033_CN_CCTV13_新闻1+1037.wav 2018-01-26_2033_CN_CCTV13_新闻1+1091.wav 2018-01-26_2033_CN_CCTV13_新闻1+1145.wav2018-01-26_2033_CN_CCTV13_新闻1+1038.wav 2018-01-26_2033_CN_CCTV13_新闻1+1092.wav 2018-01-26_2033_CN_CCTV13_新闻1+1146.wav2018-01-26_2033_CN_CCTV13_新闻1+1039.wav 2018-01-26_2033_CN_CCTV13_新闻1+1093.wav 2018-01-26_2033_CN_CCTV13_新闻1+1147.wav2018-01-26_2033_CN_CCTV13_新闻1+1040.wav 2018-01-26_2033_CN_CCTV13_新闻1+1094.wav 2018-01-26_2033_CN_CCTV13_新闻1+1148.wav2018-01-26_2033_CN_CCTV13_新闻1+1041.wav 2018-01-26_2033_CN_CCTV13_新闻1+1095.wav 2018-01-26_2033_CN_CCTV13_新闻1+1149.wav2018-01-26_2033_CN_CCTV13_新闻1+1042.wav 2018-01-26_2033_CN_CCTV13_新闻1+1096.wav 2018-01-26_2033_CN_CCTV13_新闻1+1150.wav2018-01-26_2033_CN_CCTV13_新闻1+1043.wav 2018-01-26_2033_CN_CCTV13_新闻1+1097.wav 2018-01-26_2033_CN_CCTV13_新闻1+1151.wav2018-01-26_2033_CN_CCTV13_新闻1+1044.wav 2018-01-26_2033_CN_CCTV13_新闻1+1098.wav 2018-01-26_2033_CN_CCTV13_新闻1+1152.wav2018-01-26_2033_CN_CCTV13_新闻1+1045.wav 2018-01-26_2033_CN_CCTV13_新闻1+1099.wav 2018-01-26_2033_CN_CCTV13_新闻1+1153.wav2018-01-26_2033_CN_CCTV13_新闻1+1046.wav 2018-01-26_2033_CN_CCTV13_新闻1+1100.wav 2018-01-26_2033_CN_CCTV13_新闻1+1154.wav2018-01-26_2033_CN_CCTV13_新闻1+1047.wav 2018-01-26_2033_CN_CCTV13_新闻1+1101.wav 2018-01-26_2033_CN_CCTV13_新闻1+1155.wav2018-01-26_2033_CN_CCTV13_新闻1+1048.wav 2018-01-26_2033_CN_CCTV13_新闻1+1102.wav 2018-01-26_2033_CN_CCTV13_新闻1+1156.wav2018-01-26_2033_CN_CCTV13_新闻1+1049.wav 2018-01-26_2033_CN_CCTV13_新闻1+1103.wav 2018-01-26_2033_CN_CCTV13_新闻1+1157.wav2018-01-26_2033_CN_CCTV13_新闻1+1050.wav 2018-01-26_2033_CN_CCTV13_新闻1+1104.wav 2018-01-26_2033_CN_CCTV13_新闻1+1158.wav2018-01-26_2033_CN_CCTV13_新闻1+1051.wav 2018-01-26_2033_CN_CCTV13_新闻1+1105.wav 2018-01-26_2033_CN_CCTV13_新闻1+1159.wav2018-01-26_2033_CN_CCTV13_新闻1+1052.wav 2018-01-26_2033_CN_CCTV13_新闻1+1106.wav 2018-01-26_2033_CN_CCTV13_新闻1+1160.wav2018-01-26_2033_CN_CCTV13_新闻1+1053.wav 2018-01-26_2033_CN_CCTV13_新闻1+1107.wav 2018-01-26_2033_CN_CCTV13_新闻1+1161.wav2018-01-26_2033_CN_CCTV13_新闻1+1054.wav 2018-01-26_2033_CN_CCTV13_新闻1+1108.wav 2018-01-26_2033_CN_CCTV13_新闻1+1162.wav Thus, we have done all the preparation. The next step is to infer the results from our model. Due to some CUDA problems, It’s still on the progress. Thanks for your reading!]]></content>
      <categories>
        <category>GSOC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Write a Singularity Recipe]]></title>
    <url>%2F2018%2F06%2F22%2F%2Farchivers%2FWrite-a-Singularity-Recipe%2F</url>
    <content type="text"><![CDATA[I met singularity when I was doing GSOC task for PaddlePaddle:DeepSpeech. So here I will show you how to build a custom singularity image from scratch. What’s the Singularity RecipeFirstly, let me briefly summarize what’s the singularity recipe and what would it help? It’s similarly with the shell script or makefile which we write in the linux/unix to help us run linux command automatically. A singularity recipe helps us build the image by running those installing commands in the Singularity Hub environment. For instance, when we add apt install vim in the recipe, it will install the vim for your environment or what we say, your image. So you can always copy all the commands you will do that set up the satisfying environment into the recipe to build an image by uploading it to Singularity Hub. How to write a Singularity RecipeI think the official docs explained it clearly so I won’t repeat, but pay attention that if you’d like to modify a singularity image or a docker image to fit your demand, (which is very common and recommended), just use:12Bootstrap:docker From:ubuntu:latest ubuntu:latest is an example,we could change with:From:paddlepaddle/deep_speech:latest-gpu so that we could inherit all the tools and pacages in that image and add other tools to help with our project. How to upload it?So how could we build our DeepSpeech image to help us with our work? Just as mentioned in the Build the container, we should create a new Github repository, and push the Singularity recipe to the repository, and then we connect the repository with our singularity hub account, and it’s done. Really easy! How to download the image and use ittake my image as an example: just runsingularity pull shub://xuzhaoqing/DeepSpeechafter we download it correctly, we could jump into the image by typingsingularity singularity shell -e &lt;image name&gt; Thanks for your reading! Hope you could get some intuition from this article.]]></content>
      <categories>
        <category>GSOC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prepare the Data]]></title>
    <url>%2F2018%2F06%2F22%2F%2Farchivers%2FPrepare-the-Data%2F</url>
    <content type="text"><![CDATA[Get into the singularity image(you could just skip this part because it has been shown by Shuwei) Notice that we don’t need to get into the image in this article Firstly, we need to download the deepspeech image from shub://xuzhaoqing/DeepSpeech by typing:1singularity pull shub://xuzhaoqing/DeepSpeech Then we rename its name to deepspeech2.simg (for convenience) by typing:1mv xuzhaoqing-DeepSpeech-master-latest.simg deepspeech2.simg Then I tried to use the command like Shuwei’s, but I failed:1234[zxx302@gpu030t zxx302]$ singularity shell -e -H /mnt/rds/redhen/gallina/Singularity/DeepSpeech2/DeepSpeech/ deepspeech2.simg ERROR : Home directory is not owned by calling user: /mnt/rds/redhen/gallina/Singularity/DeepSpeech2/DeepSpeech/ABORT : Retval = 255 So I did the processing on my home directory, which is zxx302. But anyway it doesn’t matter. I then clone DeepSpeech to my directory using:1git clone https://github.com/PaddlePaddle/DeepSpeech.git Data FormFrom today on we’re going to prepare the data for the inputs of the deepspeech model. As you can see in the data shown by Baidu, the data input should be splited into pieces, each with 3-7 seconds. It’s really easy to understand because the technologies like Markov chain, RNN and GRU couldn’t “remember” too long sentences. So we have to split our 30 minutes’ data to 5 seconds. Take one small data sample for intuition:1234BAC009S0658W0121.wav BAC009S0658W0242.wav BAC009S0658W0375.wavBAC009S0658W0122.wav BAC009S0658W0243.wav BAC009S0658W0376.wavBAC009S0658W0123.wav BAC009S0658W0244.wav BAC009S0658W0377.wavBAC009S0658W0124.wav BAC009S0658W0245.wav BAC009S0658W0378.wav As you can see, BAC009S0658W0 is previously a long audio, and it’s splited by a tool called sox into nearly 500 pieces. Split the audiotransform the video to audioby typing the code shown, we could transform the video to audio1ffmpeg -i 2018-04-30_1100_CN_CCTV1_新闻联播.mp4 2018-04-30_1100_CN_CCTV1_新闻联播.wav split itwe then create a folder called split to store the pieces:1mkdir split and use sox to split the audio1find . -name &apos;*.wav&apos; -exec sh -c &apos;mkdir -p ./split/$(dirname &quot;&#123;&#125;&quot;)&apos; \; -exec sox &#123;&#125; ./split/&#123;&#125; trim 0 10 : newfile : restart \; Here, as you can see, 10 means the seconds you’d like to split,here I choose 10 I have copyed 2018-04-30_1100_CN_CCTV1_新闻联播.wav to /mnt/rds/redhen/gallina/Singularity/Zhaoqing/ so that any of you could try following the steps Next stepThere are two parts of tasks for us to solve: one is to test if the data works for the model, the other is to write runscripts to process all the data. We will be in progress soon. Any suggestions?]]></content>
      <categories>
        <category>GSOC</category>
      </categories>
  </entry>
</search>
